
MindBrige_text_only_mixco_loss starting with epoch 0 / 300
  0%|                                                                                                                                   | 0/300 [00:00<?, ?it/s]
>>> Epoch0 | Iter0 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter1 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter2 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter3 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter4 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter5 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter6 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter7 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter8 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter9 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter10 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter11 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter12 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter13 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter14 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter15 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter16 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter17 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter18 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter19 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter20 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter21 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter22 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter23 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter24 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter25 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter26 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter27 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter28 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter29 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter30 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter31 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter32 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter33 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter34 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter35 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter36 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter37 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter38 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter39 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter40 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter41 | voxel: torch.Size([800, 8192])
>>> Epoch0 | Iter42 | voxel: torch.Size([636, 8192])
train/loss: 56.48270833215048
train/lr: 0.00015333903768588529
train/num_steps: 43
train/cosine_sim_text: 0.15587319128031216
train/loss_nce_text: 6.839737958686296
train/loss_mse_text: 2.8548659535116776e-05
train/loss_mae_text: 0.003796008399276193
train/loss_rec: 1.0783028519430826
train/loss_cyc: 0.9099796435167623
Evaluating...
>>> Epoch0 | Eval0 | voxel: torch.Size([200, 8192])
>>> Epoch0 | Eval1 | voxel: torch.Size([200, 8192])
>>> Epoch0 | Eval2 | voxel: torch.Size([200, 8192])
>>> Epoch0 | Eval3 | voxel: torch.Size([200, 8192])
>>> Epoch0 | Eval4 | voxel: torch.Size([200, 8192])

  0%|▎                                                                                       | 1/300 [00:58<4:52:46, 58.75s/it, epoch=0, lr=0.000153, loss=56.5]
val/loss: 45.01204617818197
val/num_steps: 6
val/cosine_sim_text: 0.2553778539101283
val/loss_nce_text: 5.205943822860718
val/loss_mse_text: 2.5183377450351447e-05
val/loss_mae_text: 0.0031620614075412354
val/loss_rec: 0.4183552910884221
val/loss_cyc: 0.04285118170082569
>>> Epoch1 | Iter0 | voxel: torch.Size([800, 8192])
>>> Epoch1 | Iter1 | voxel: torch.Size([800, 8192])
>>> Epoch1 | Iter2 | voxel: torch.Size([800, 8192])
>>> Epoch1 | Iter3 | voxel: torch.Size([800, 8192])
>>> Epoch1 | Iter4 | voxel: torch.Size([800, 8192])
>>> Epoch1 | Iter5 | voxel: torch.Size([800, 8192])
  0%|▎                                                                                       | 1/300 [01:14<6:10:50, 74.42s/it, epoch=0, lr=0.000153, loss=56.5]
Traceback (most recent call last):
  File "/media/SSD_1_2T/xt/MindBridge/src/main.py", line 195, in <module>
    main()
  File "/media/SSD_1_2T/xt/MindBridge/src/main.py", line 189, in main
    trainer.train(local_rank)
  File "/media/SSD_1_2T/xt/MindBridge/src/trainer_fmri_text.py", line 160, in train
    self.train_epoch(epoch)
  File "/media/SSD_1_2T/xt/MindBridge/src/trainer_fmri_text.py", line 645, in train_epoch
    self.train_step(voxel, image, captions, subj_id, epoch)
  File "/media/SSD_1_2T/xt/MindBridge/src/trainer_fmri_text.py", line 198, in train_step
    clip_text = self.clip_extractor.embed_text(captions).float()
  File "/media/SSD_1_2T/xt/MindBridge/src/models.py", line 162, in embed_text
    untruncated_ids = self.tokenizer(prompt, padding = "max_length", return_tensors = "pt").input_ids
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2883, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2969, in _call_one
    return self.batch_encode_plus(
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3160, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 803, in _batch_encode_plus
    first_ids = get_input_ids(ids)
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 770, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 617, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/transformers/models/clip/tokenization_clip.py", line 467, in _tokenize
    for token in re.findall(self.pat, text):
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/regex/regex.py", line 337, in findall
    pat = _compile(pattern, flags, ignore_unused, kwargs, True)
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/site-packages/regex/regex.py", line 466, in _compile
    pattern_locale = _getpreferredencoding()
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/locale.py", line 666, in getpreferredencoding
    old_loc = setlocale(LC_CTYPE)
  File "/home/fmri/miniconda3/envs/xt/lib/python3.10/locale.py", line 620, in setlocale
    return _setlocale(category, locale)
KeyboardInterrupt